{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5539ea3e-08f2-413d-a040-c5bf093c7243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 --> NVIDIA RTX A5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-24 11:31:17.409575: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-24 11:31:17.433183: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-24 11:31:17.433203: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-24 11:31:17.433812: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-24 11:31:17.437626: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pickle\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# !pip install git+https://github.com/huggingface/transformers.git\n",
    "# !pip install accelerate bitsandbytes xformers\n",
    "\n",
    "from huggingface_hub import notebook_login\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "# notebook_login()\n",
    "for i in range(torch.cuda.device_count()):\n",
    "   print(i, \"-->\", torch.cuda.get_device_properties(i).name)\n",
    "\n",
    "from torch import bfloat16\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6fa6e6d-a5b8-461e-8f04-2ebee7fb55aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02f31cb3f9a242d2b86e7f5ef01b11c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\", \n",
    "    # model=\"HuggingFaceH4/zephyr-7b-beta\", \n",
    "    model=\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    # model=\"meta-llama/Llama-2-7b-hf\",\n",
    "    # model=\"meta-llama/Llama-2-13b-chat-hf\",\n",
    "    # model=\"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "    torch_dtype=bfloat16, \n",
    "    # device_map=\"auto\"\n",
    "    # batch_size=1,\n",
    "    device=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ee3600b-8dab-4bec-a1b5-d47722d5d212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115537 sentences found\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/local/sps-local/docs/a5/57/6e/4e/a5576e4e89fc1ad271ba00b3694fde1d63a95e77d5f85102410dbbd65e95c05d.json',\n",
       " 1,\n",
       " 1185,\n",
       " [(1, 31, 48), (1, 75, 91), (3, 148, 154)],\n",
       " 'Based on our experience in the β- and γ-C(sp3)-H\\nfunctionalization of free carboxylic acids, we thus expected that\\nthe identification of a suitable ligand would be crucial for the\\ndevelopment of the desired alkynylation process.\\n\\n')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "with open(\"/local/sps-local/ner-role-extraction/relevant_sentences.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "print(f\"{len(data)} sentences found\")\n",
    "data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58202d92-f81c-433f-ae00-37ed8f40f5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYS_PROMPT = ('Do you agree with the provided question? Please answer with one word, either \"yes\" or \"no\".')\n",
    "# SYS_PROMPT = ('Do you agree with the provided question? Please answer with \"yes\" or \"no\".')\n",
    "# SYS_PROMPT = ('Do you agree with the question? Please answer using one word.')\n",
    "# SYS_PROMPT = ('Your job is to answer my questions.')\n",
    "# SYS_PROMPT = ('Your job is to answer my questions simply using yes or no.')\n",
    "# SYS_PROMPT = ('')\n",
    "\n",
    "def answer(sentence, chemical, role):\n",
    "    # input = f'According to the sentence \"{sentence}\": Is {chemical} explicitly described as a {role}?'\n",
    "    input = f'In the sentence \"{sentence}\": Is {chemical} explicitly described as {role}?'\n",
    "    \n",
    "    USER_PROMPT = f\"context: ```{input}``` \\n\\n output: \"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": SYS_PROMPT\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": USER_PROMPT\n",
    "        },\n",
    "    ]\n",
    "    prompt = pipe.tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    outputs = pipe(\n",
    "        prompt,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=True,\n",
    "        temperature=0.1,\n",
    "        top_p=0.95\n",
    "    )\n",
    "    ret = outputs[0][\"generated_text\"]\n",
    "    # print(ret)\n",
    "    if (ret[ret.find(\"[/INST]\") + 9: ]).lower().startswith(\"yes\"):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2606a026-900b-4955-9e3e-2051483b8241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/local/sps-local/docs/a5/57/23/d8/a55723d8437655575840c05dbe343a183ace18439654cf6b8e408f62cd8aadcb.json',\n",
       " 12,\n",
       " 131,\n",
       " [(3, 60, 69), (1, 279, 282)],\n",
       " 'By using the time (t1) of the first switching crystal as an indicator that the gas has filled the\\nchannel we can compare directly the induction time for the following crystals (tn) by defining\\nthe normalized switching time 𝑡̂ :\\n𝑡̂\\n\\n𝑡\\n\\n𝑡\\n\\n(Eq. 2)\\n\\nDecreasing the concentration of DCM vapor from 100 to 60%, the distribution of normalized\\ninduction time broadens.')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10959acc-a053-40eb-af46-d970835c10ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                    | 6/115537 [00:01<6:25:59,  4.99it/s]/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      " 45%|████████████████████████████████████████████████████████████████▏                                                                             | 52255/115537 [4:49:50<5:49:23,  3.02it/s]"
     ]
    }
   ],
   "source": [
    "def normalize_whitespaces(s):\n",
    "    return \" \".join(s.split()).strip()\n",
    "\n",
    "def process_context(context):\n",
    "    context_filepath, context_page, context_pos, context_occurences, context_sentence = context    \n",
    "    chemicals = set()\n",
    "    roles = set()\n",
    "    \n",
    "    for o in context_occurences:\n",
    "        entity = normalize_whitespaces(context_sentence[o[1]:o[2]])\n",
    "        if o[0] == 1:\n",
    "            chemicals.add(entity)\n",
    "        else:\n",
    "            roles.add(entity.lower())\n",
    "\n",
    "    true_relations = []\n",
    "    false_relations = []\n",
    "    \n",
    "    for c in chemicals:        \n",
    "        for r in roles:\n",
    "            if answer(context_sentence.replace(\"\\n\",\" \"), c, r):\n",
    "                true_relations.append((c, r, context_filepath, context_page, context_pos))\n",
    "            else:\n",
    "                false_relations.append((c, r, context_filepath, context_page, context_pos))\n",
    "    return true_relations, false_relations\n",
    "\n",
    "true_relations, false_relations = [], []\n",
    "\n",
    "def pickle_relations(true_relations, false_relations):\n",
    "    with open(\"/local/sps-local/ner-role-extraction/true_relations.pkl\", \"wb\") as f:\n",
    "        pickle.dump(true_relations, f)\n",
    "    with open(\"/local/sps-local/ner-role-extraction/false_relations.pkl\", \"wb\") as f:\n",
    "        pickle.dump(false_relations, f)\n",
    "\n",
    "\n",
    "i = 0\n",
    "with open(\"/local/sps-local/ner-role-extraction/llama2-role-validator.log\", \"w\") as log:\n",
    "    log.write(f\"{datetime.now()}: {i} of {len(data)}\\n\")\n",
    "    log.flush()\n",
    "    for context in tqdm(data):\n",
    "        i += 1\n",
    "    \n",
    "        if i%1000 == 0:\n",
    "            pickle_relations(true_relations, false_relations)\n",
    "            log.write(f\"{datetime.now()}: {i} of {len(data)}\\n\")\n",
    "            log.flush()\n",
    "            \n",
    "        # print(context)\n",
    "        tr, fr = process_context(context)\n",
    "        true_relations.extend(tr)\n",
    "        false_relations.extend(fr)\n",
    "\n",
    "true_relations[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1fd2fc-49be-46b4-83f6-04ead779caf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "false_relations[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87a833e-0e9d-4f93-af90-502201c05bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "roles_to_chems = {} # dict of <role, dict<chems, [positional information]>>\n",
    "\n",
    "for chem, role, context_filepath, context_page, context_pos in true_relations:\n",
    "    chems = roles_to_chems.get(role, {})\n",
    "    pos = chems.get(chem, [])\n",
    "    pos.append((context_filepath, context_page, context_pos))\n",
    "    chems[chem]=pos\n",
    "    roles_to_chems[role]=chems\n",
    "\n",
    "for role in roles_to_chems:\n",
    "    chems = roles_to_chems.get(role)\n",
    "    for chem in chems:\n",
    "        pos = chems[chem] \n",
    "        count = len(pos)\n",
    "        if count > 1:\n",
    "            print(role, \"|\", chem, \"->\", count)        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa884f0-5180-415b-8687-3fa2f6e6eb72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeab1b15-45c5-4e39-84d3-ac65b96db5b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
