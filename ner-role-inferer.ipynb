{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c4c563a-55e4-449d-bfaa-515528465933",
   "metadata": {},
   "source": [
    "## load the model which has been trained in the other notebook and use for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b2ee05c-1d59-48e2-a65d-f6cc288397ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.config.list_physical_devices(\"GPU\"))\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from transformers import create_optimizer\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TFAutoModelForTokenClassification\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "model_name = \"google/electra-base-discriminator\"\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, return_offsets_mapping=True)\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "model_name = \"google/electra-base-discriminator\"\n",
    "\n",
    "\n",
    "# used for common BERT model -> can retrieve chemicals and roles\n",
    "id2label = {\n",
    "    0: \"O\",\n",
    "    1: \"B-chemical\",\n",
    "    2: \"I-chemical\",\n",
    "    3: \"B-role\",\n",
    "    4: \"I-role\"\n",
    "}\n",
    "label2id = {\n",
    "    \"O\": 0,\n",
    "    \"B-chemical\": 1,\n",
    "    \"I-chemical\": 2,\n",
    "    \"B-role\": 3,\n",
    "    \"I-role\": 4,    \n",
    "}\n",
    "\n",
    "html_elems = {\n",
    "    1: (\"<b style='font-size:1.5em;'>\", \"</b>\"),\n",
    "    3: (\"<b style='color:blue; font-size:1.5em;'><i>\",\"</i></b>\")    \n",
    "}\n",
    "\n",
    "def load_model(folder, filename):\n",
    "    model = TFAutoModelForTokenClassification.from_pretrained(\n",
    "        os.path.join(folder, filename), num_labels=len(id2label.keys()), id2label=id2label, label2id=label2id\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b117e19f-8000-4b17-a2ea-dc40c03c5f18",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFElectraForTokenClassification.\n",
      "\n",
      "All the layers of TFElectraForTokenClassification were initialized from the model checkpoint at ./model/chemical_extract_google-electra-base-discriminator.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFElectraForTokenClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: <transformers.models.electra.modeling_tf_electra.TFElectraForTokenClassification object at 0x7fd561b50d50>\n",
      "Model: \"tf_electra_for_token_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " electra (TFElectraMainLaye  multiple                  108891648 \n",
      " r)                                                              \n",
      "                                                                 \n",
      " dropout_37 (Dropout)        multiple                  0 (unused)\n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  3845      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 108895493 (415.40 MB)\n",
      "Trainable params: 108895493 (415.40 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model = load_model(\"./model\", f\"chemical_substances_roles_extract_{model_name.replace('/', '-')}\")\n",
    "model = load_model(\"./model\", f\"chemical_extract_{model_name.replace('/', '-')}\")\n",
    "print(\"model:\", model)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "872ee67f-ca15-4d79-8421-0f6781699600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b style='font-size:1.5em;'></b>Due to the aprotic nature the <b style='color:blue; font-size:1.5em;'><i>solvent</i></b> with not be able to form <b style='font-size:1.5em;'>hydrogen</b> bond with <b style='font-size:1.5em;'>Pro</b> and <b style='font-size:1.5em;'>Hyp</b> Interstitially, when we assessed self-assembling behavior of <b style='font-size:1.5em;'>Pro</b> and <b style='font-size:1.5em;'>Hyp</b> \n",
       "in this <b style='color:blue; font-size:1.5em;'><i>solvent</i></b> system we could not assess any structure formation for both <b style='font-size:1.5em;'>Pro</b> and <b style='font-size:1.5em;'>Hyp</b>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import TFAutoModelForTokenClassification\n",
    "import json\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "\n",
    "\n",
    "# return spans with start and end index and a class\n",
    "def infer_text(model, tokenizer, text):\n",
    "    specials = []\n",
    "        \n",
    "    tokenized = tokenizer(text, return_tensors=\"tf\")\n",
    "    offset_mapping = tokenizer(text, return_offsets_mapping=True)[\"offset_mapping\"]    \n",
    "    \n",
    "    logits = model(**tokenized).logits\n",
    "    predicted_probs = tf.nn.softmax(logits, axis=2)    \n",
    "    \n",
    "    predicted_token_class_ids = tf.math.argmax(logits, axis=-1)[0]  \n",
    "    predicted_token_class = [model.config.id2label[t] for t in predicted_token_class_ids.numpy().tolist()]\n",
    "    probs = tf.math.reduce_max(predicted_probs, axis=-1)\n",
    "\n",
    "    # for l, t in zip(predicted_token_class_ids, tokenizer.convert_ids_to_tokens(tokenized[\"input_ids\"][0])):        \n",
    "    #     print(id2label[int(l)], t)\n",
    "\n",
    "    label, start, end = 0, 0, 0    \n",
    "\n",
    "    i = 0\n",
    "    while i < len(predicted_token_class_ids):\n",
    "        l = predicted_token_class_ids[i]\n",
    "        if l % 2 == 1:\n",
    "            label = int(l)\n",
    "            start = offset_mapping[i][0]\n",
    "            while i+1 < len(predicted_token_class_ids) and predicted_token_class_ids[i + 1] == l+1:\n",
    "                i += 1\n",
    "            end = offset_mapping[i][1]\n",
    "            specials.append((label, start, end))\n",
    "        i += 1\n",
    "\n",
    "    return specials\n",
    "\n",
    "def render_as_html(text, specials):\n",
    "    start_dict = {}\n",
    "    end_dict = {}\n",
    "    for s in specials:\n",
    "        start_dict[s[1]] = html_elems[s[0]][0]\n",
    "        end_dict[s[2]] = html_elems[s[0]][1]\n",
    "    html = \"\"    \n",
    "    for i, c  in enumerate(text):                \n",
    "        start_elem = start_dict.get(i, None)\n",
    "        end_elem = end_dict.get(i, None)\n",
    "\n",
    "        if start_elem:\n",
    "            html += start_elem\n",
    "        if end_elem:\n",
    "            html += end_elem\n",
    "        html += c\n",
    "    display(HTML(html))\n",
    "\n",
    "text = \"\"\"Due to the aprotic nature the solvent with not be able to form hydrogen bond with Pro and Hyp Interstitially, when we assessed self-assembling behavior of Pro and Hyp \n",
    "in this solvent system we could not assess any structure formation for both Pro and Hyp.\"\"\"\n",
    "\n",
    "specials = infer_text(model, tokenizer, text)\n",
    "render_as_html(text, specials)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03d84d9f-102c-4d28-bc53-11e3fae9c282",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'based', 'on', 'our', 'experience', 'in', 'the', 'β', '-', 'and', 'γ', '-', 'c', '(', 'sp', '##3', ')', '-', 'h', 'functional', '##ization', 'of', 'free', 'car', '##box', '##yl', '##ic', 'acids', ',', 'we', 'thus', 'expected', 'that', 'the', 'identification', 'of', 'a', 'suitable', 'ligand', 'would', 'be', 'crucial', 'for', 'the', 'development', 'of', 'the', 'desired', 'al', '##ky', '##ny', '##lation', 'process', '.', \"'\", '[SEP]']\n",
      "[(1, 0, 0), (1, 31, 48), (1, 75, 91), (3, 148, 154), (1, 0, 0)]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<b style='font-size:1.5em;'></b>Based on our experience in the <b style='font-size:1.5em;'>β- and γ-C(sp3)-H</b>\n",
       "functionalization of free <b style='font-size:1.5em;'>carboxylic acids</b>, we thus expected that\n",
       "the identification of a suitable <b style='color:blue; font-size:1.5em;'><i>ligand</i></b> would be crucial for the\n",
       "development of the desired alkynylation process.\n",
       "\n",
       "'"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "text = \"\"\"Based on our experience in the β- and γ-C(sp3)-H\\nfunctionalization of free carboxylic acids, we thus expected that\\nthe identification of a suitable ligand would be crucial for the\\ndevelopment of the desired alkynylation process.\\n\\n'\"\"\"\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenizer(text, return_offsets_mapping=True)[\"input_ids\"])\n",
    "print(tokens)\n",
    "classifier = pipeline(\"ner\", tokenizer=tokenizer, model=model)\n",
    "classifier(text)[0:3]\n",
    "\n",
    "specials = infer_text(model, tokenizer, text)\n",
    "print(specials)\n",
    "render_as_html(text, specials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5252ac90-d680-467e-979d-9752714ff3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Atomic <b style='font-size:1.5em;'>Fe</b> in N-doped <b style='font-size:1.5em;'>carbon</b> (FeNC) electrocatalysts for <b style='font-size:1.5em;'>oxygen</b> (<b style='font-size:1.5em;'>O2</b>) reduction at the cathode of proton exchange membrane fuel cells (PEMFCs) \n",
       "are the most promising alternative to <b style='font-size:1.5em;'>platinum</b>-group-metal catalysts. Despite recent progress on atomic FeNC <b style='font-size:1.5em;'>O2</b> reduction, their controlled synthesis and stability \n",
       "for practical applications remains challenging. A two-step synthesis approach has recently led to significant advances in terms of <b style='font-size:1.5em;'>Fe</b>-loading and mass activity; \n",
       "however, the <b style='font-size:1.5em;'>Fe</b> utilisation remains low owing to the difficulty of building scaffolds with sufficient porosity that electrochemically exposes the active sites. \n",
       "Herein, we addressed this issue by coordinating <b style='font-size:1.5em;'>Fe</b> in a highly porous <b style='font-size:1.5em;'>nitrogen</b> doped <b style='font-size:1.5em;'>carbon</b> support (~3295 m2 g-1), prepared by pyrolysis of inexpensive \n",
       "<b style='font-size:1.5em;'>2,4,6-triaminopyrimidine</b> and a <b style='font-size:1.5em;'>Mg2+</b> salt active site template and porogen. Upon <b style='font-size:1.5em;'>Fe</b> coordination, a high electrochemical active site density of 2.54×10^19 \n",
       "sites gFeNC-1 and a record 52% FeNx electrochemical utilisation based on in situ <b style='font-size:1.5em;'>nitrite</b> stripping was achieved. The <b style='font-size:1.5em;'>Fe</b> single atoms are characterised pre- \n",
       "and post-electrochemical accelerated stress testing by aberration-corrected high-angle annular dark field scanning transmission electron microscopy, showing no <b style='font-size:1.5em;'>Fe</b> \n",
       "clustering. Moreover, ex situ X-ray absorption spectroscopy and low-temperature Mössbauer spectroscopy suggest the presence of penta-coordinated <b style='font-size:1.5em;'>Fe</b> sites, which \n",
       "were further studied by density functional theory calculations.\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"\\n\\n\\n\")\n",
    "text = \"\"\"Atomic Fe in N-doped carbon (FeNC) electrocatalysts for oxygen (O2) reduction at the cathode of proton exchange membrane fuel cells (PEMFCs) \n",
    "are the most promising alternative to platinum-group-metal catalysts. Despite recent progress on atomic FeNC O2 reduction, their controlled synthesis and stability \n",
    "for practical applications remains challenging. A two-step synthesis approach has recently led to significant advances in terms of Fe-loading and mass activity; \n",
    "however, the Fe utilisation remains low owing to the difficulty of building scaffolds with sufficient porosity that electrochemically exposes the active sites. \n",
    "Herein, we addressed this issue by coordinating Fe in a highly porous nitrogen doped carbon support (~3295 m2 g-1), prepared by pyrolysis of inexpensive \n",
    "2,4,6-triaminopyrimidine and a Mg2+ salt active site template and porogen. Upon Fe coordination, a high electrochemical active site density of 2.54×10^19 \n",
    "sites gFeNC-1 and a record 52% FeNx electrochemical utilisation based on in situ nitrite stripping was achieved. The Fe single atoms are characterised pre- \n",
    "and post-electrochemical accelerated stress testing by aberration-corrected high-angle annular dark field scanning transmission electron microscopy, showing no Fe \n",
    "clustering. Moreover, ex situ X-ray absorption spectroscopy and low-temperature Mössbauer spectroscopy suggest the presence of penta-coordinated Fe sites, which \n",
    "were further studied by density functional theory calculations.\n",
    "\"\"\"\n",
    "specials = infer_text(model, tokenizer, text)\n",
    "render_as_html(text, specials)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0691d4c-4e8e-43bb-bda2-89cde4252fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Direct β- and γ-C(sp3)–H Alkynylation of Free <b style='font-size:1.5em;'>Carboxylic Acids</b>\n",
       "Francesca Ghiringhelli,[a] Manuel van Gemmeren*[a]\n",
       "\n",
       "[a]\n",
       "\n",
       "F."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"\\n\\n\\n\")\n",
    "text = \"\"\"Direct β- and γ-C(sp3)–H Alkynylation of Free Carboxylic Acids\\nFrancesca Ghiringhelli,[a] Manuel van Gemmeren*[a]\\n\\n[a]\\n\\nF.\"\"\"\n",
    "specials = infer_text(model, tokenizer, text)\n",
    "render_as_html(text, specials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45c22459-0f9e-4892-9aa3-bf455caf3944",
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHED_ARTICLES_DIR = \"/local/sps-local/docs\"\n",
    "\n",
    "# read json document and return content as a json object\n",
    "def get_json_from_file(json_file):\n",
    "    with open(json_file, \"r\") as f:\n",
    "        return json.loads(f.read())\n",
    "    \n",
    "def recursively_collect_files():\n",
    "    filepaths = []\n",
    "    for root, dirs, files in os.walk(CACHED_ARTICLES_DIR):\n",
    "        for filename in files:            \n",
    "            if filename.endswith(\".json\") and not filename.endswith(\"-substances.json\"):                   \n",
    "                filepaths.append(os.path.join(root, filename))\n",
    "    return filepaths\n",
    "\n",
    "\n",
    "# def recursively_collect_sentences():\n",
    "#     sentences_words, role_labels = [], []\n",
    "#     file_counter = 0\n",
    "#     for root, dirs, files in os.walk(CACHED_ARTICLES_DIR):\n",
    "#         for filename in files:            \n",
    "#             if filename.endswith(\".json\") and not filename.endswith(\"-substances.json\"):                                \n",
    "#                 file_counter += 1\n",
    "#                 if file_counter % 10 == 0:\n",
    "#                     print(f\"collected sentences from {file_counter} files.\")\n",
    "#                 json_data = get_json_from_file(os.path.join(root, filename))\n",
    "#                 filehash = json_data[\"fileHash\"]\n",
    "#                 contenthash = json_data[\"contentHash\"]\n",
    "#                 texthash = json_data[\"textHash\"]\n",
    "#                 origpath = json_data[\"filepath\"]\n",
    "    \n",
    "#                 pages = [page for page in json_data[\"pages\"]]\n",
    "                  \n",
    "#                 for page in pages:\n",
    "#                     doc = nlp(page[\"text\"])\n",
    "#                     for sentence in doc.sents:                        \n",
    "#                         pageNumber = int(page[\"pageNumber\"])\n",
    "#                         infer(model, tokenizer, sentence.text)\n",
    "    \n",
    "#     return filename, origpath, sentences_words\n",
    "\n",
    "# recursively_collect_sentences()\n",
    "\n",
    "filepaths = recursively_collect_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "138dfb43-e328-490f-bf6f-08744a18ab45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_relevant_sentences(filepath):    \n",
    "    \"\"\"\n",
    "    collect sentences which have at least one chemical and one role\n",
    "    \"\"\"\n",
    "    sentences = []    \n",
    "    json_data = get_json_from_file(filepath)\n",
    "    filehash = json_data[\"fileHash\"]\n",
    "    contenthash = json_data[\"contentHash\"]\n",
    "    texthash = json_data[\"textHash\"]\n",
    "    origpath = json_data[\"filepath\"]\n",
    "\n",
    "    pages = [page for page in json_data[\"pages\"]]\n",
    "    \n",
    "    for page in pages:\n",
    "        doc = nlp(page[\"text\"])\n",
    "        for sentence in doc.sents:                        \n",
    "            page_number = int(page[\"pageNumber\"])\n",
    "            specials = infer_text(model, tokenizer, sentence.text)\n",
    "            contains_chem = False\n",
    "            contains_role = False\n",
    "            for s in specials:\n",
    "                if s[0] == 1:\n",
    "                    contains_chem = True\n",
    "                if s[0] % 2 == 1 and s[0] > 1:\n",
    "                    contains_role = True\n",
    "            if contains_chem and contains_role:                \n",
    "                sentences.append((filepath, page_number, doc[sentence.start].idx, specials, sentence.text))\n",
    "    return sentences\n",
    "     \n",
    "\n",
    "def load_relevant_sentences():\n",
    "    if os.path.isfile(\"/local/sps-local/ner-role-extraction/relevant_sentences.pkl\"):\n",
    "        with open(\"/local/sps-local/ner-role-extraction/relevant_sentences.pkl\", \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "def pickle_relevant_sentences():\n",
    "    with open(\"/local/sps-local/ner-role-extraction/relevant_sentences.pkl\", \"wb\") as f:\n",
    "        pickle.dump(sentences, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1a725e-82b7-42da-aead-5f363e1f3515",
   "metadata": {},
   "source": [
    "# offset and limit for training\n",
    "\n",
    "roughly 1000 files take 10 hours and 10 minutes\n",
    "there is probably a memory leak as the process freezes after 1950 documents have been processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fba3c8f-2bde-47ef-849d-664bec487572",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                                                                                                                    | 2/1200 [01:13<12:07:47, 36.45s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (529 > 512). Running this sequence through the model will result in indexing errors\n",
      " 28%|█████████████████████████████████████████▏                                                                                                        | 339/1200 [3:24:50<8:14:14, 34.44s/it]"
     ]
    }
   ],
   "source": [
    "offset = 7000\n",
    "limit = 1000\n",
    "sentences = load_relevant_sentences()\n",
    "\n",
    "with open(\"/local/sps-local/ner-role-extraction/ner-role-inferer.log\", \"w\") as log:    \n",
    "    log.write(f\"{datetime.now()}: starting at offset {offset} and stopping at {offset+limit}\\n\")\n",
    "    log.flush()\n",
    "    for filepath in tqdm(filepaths[offset:offset+limit]):    \n",
    "        offset += 1\n",
    "        if offset%10 == 0:\n",
    "            log.write(f\"{datetime.now()}: attempting saving {offset}\\n\")\n",
    "            log.flush()\n",
    "            pickle_relevant_sentences()\n",
    "            log.write(f\"{datetime.now()}: done {offset}\\n\")\n",
    "            log.flush()\n",
    "        sentences.extend(collect_relevant_sentences(filepath))\n",
    "\n",
    "pickle_relevant_sentences()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32542baa-d63b-413e-868a-32edd190b6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Biomolecules in microbes related to CO2 -sensitive pathways or acting as a CO2 trans-\n",
    "ducer have been proposed as appealing targets for medicines, since they control cell devel-\n",
    "opment and the subsequent synthesis of chemicals, enhancing the pathogen persistence\n",
    "in the host [26,27]. In this context, a crucial role is played by a superfamily of molecules\n",
    "known as carbonic anhydrases (CAs, EC 4.2.1.1). CAs can be thought as molecules that,\n",
    "rather than instantly detecting a change in CO2 , serve as CO2 transducers, adjusting its\n",
    "levels [23,28]. With their activity, the CAs encoded by the bacterial genome of pathogenic\n",
    "and non-pathogenic bacteria provide the indispensable CO2 and HCO3 − /protons to micro-\n",
    "bial biosynthetic pathways, catalyzing the reversible reaction of CO2 hydration to HCO3 −\n",
    "and H+(CO2+H2OHCO3−+H+)\"\"\"\n",
    "specials = infer_text(model, tokenizer, text)\n",
    "render_as_html(text, specials)\n",
    "\n",
    "print()\n",
    "text = \"\"\"Moreover, LPE of pristine biochars in dimethyl carbonate, ethyl acetate, and solketal gave similar yields to more commonly used solvent for this process, N-methyl-2-pyrrolidone (NMP) a known reprotoxic molecule.\"\"\"\n",
    "specials = infer_text(model, tokenizer, text)\n",
    "render_as_html(text, specials)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af620ab-aa00-4be4-995f-fc0cba9c9156",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1071d8ec-f607-4d96-ae12-728bfb14bfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "device = cuda.get_current_device()\n",
    "device.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48397d77-f37e-417c-af69-45c4fa689a29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
