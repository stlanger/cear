{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e41d3de-967c-4710-a721-0ce703ff14e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "sys.path.append(\"./loaders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5772f685-6a2e-48da-8c33-e984a0e53e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ChebiLoader import ChebiLoader\n",
    "# chebi = ChebiLoader(\"/local/sps-local/chebi/chebi.owl\")\n",
    "\n",
    "# from BC5CDRLoader import BC5CDRLoader\n",
    "# cdr = BC5CDRLoader(\"./assets/train/BC5CDR/\", chebi)\n",
    "\n",
    "# with open(\"./assets/cdrloader.pkl\", \"wb\") as f:\n",
    "#     pickle.dump((cdr.spans, cdr.labels), f)\n",
    "\n",
    "# print(\"saved prepared bc5cdr datasets\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b82ee29-a7bc-42e1-8c14-d1e24d0bff01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from NLMChemLoader import NLMChemLoader\n",
    "# nlm = NLMChemLoader(\"./assets/train/NLM_Chem_corpus/\", chebi)\n",
    "# with open(\"./assets/nlmloader.pkl\", \"wb\") as f:\n",
    "#     pickle.dump((nlm.spans, nlm.labels), f)\n",
    "\n",
    "# print(\"saved prepared nlm datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0db38bd5-15e6-4413-a6aa-9163bff5aa8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from CraftLoader import CraftLoader\n",
    "# craft = CraftLoader(\"./assets/train/CRAFT/\", chebi)\n",
    "# with open(\"./assets/craftloader.pkl\", \"wb\") as f:\n",
    "#     pickle.dump((craft.spans, craft.labels), f)\n",
    "\n",
    "# print(\"saved prepared craft datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b5abddc-7c8d-4a27-9295-cc32ceb4ec7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cdr-spans: 9873 cdr-labels: 9873\n",
      "nlm-spans: 29782 nlm-labels: 29782\n",
      "craft-spans: 21638 craft-labels: 21638\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "cdr_spans = []\n",
    "cdr_labels = []\n",
    "\n",
    "nlm_spans = []\n",
    "nlm_labels = []\n",
    "\n",
    "craft_spans = []\n",
    "craft_labels = []\n",
    "\n",
    "\n",
    "with open(\"./assets/cdrloader.pkl\", \"rb\") as f:\n",
    "    (cdr_spans, cdr_labels) = pickle.load(f)\n",
    "print(\"cdr-spans:\", len(cdr_spans), \"cdr-labels:\", len(cdr_labels))\n",
    "\n",
    "with open(\"./assets/nlmloader.pkl\", \"rb\") as f:\n",
    "    (nlm_spans, nlm_labels) = pickle.load(f)\n",
    "print(\"nlm-spans:\", len(nlm_spans), \"nlm-labels:\", len(nlm_labels))\n",
    "\n",
    "with open(\"./assets/craftloader.pkl\", \"rb\") as f:\n",
    "    (craft_spans, craft_labels) = pickle.load(f)\n",
    "print(\"craft-spans:\", len(craft_spans), \"craft-labels:\", len(craft_labels))\n",
    "\n",
    "\n",
    "label_list = [\"O\", \"B-Chemical\", \"I-Chemical\", \"B-role\", \"I-role\"]\n",
    "\n",
    "id2label = {\n",
    "        0: \"O\",\n",
    "        1: \"B-chemical\",\n",
    "        2: \"I-chemical\",\n",
    "        3: \"B-role\",\n",
    "        4: \"I-role\"\n",
    "    }\n",
    "    \n",
    "label2id = {\n",
    "        \"O\": 0,\n",
    "        \"B-chemical\": 1,\n",
    "        \"I-chemical\": 2,\n",
    "        \"B-role\": 3,\n",
    "        \"I-role\": 4\n",
    "    }\n",
    "\n",
    "from ModelTuner import ModelTuner\n",
    "tuner = ModelTuner(\"google/electra-base-discriminator\", label_list, id2label, label2id)\n",
    "tuner.align_token_data(cdr_spans, cdr_labels)\n",
    "tuner.align_token_data(nlm_spans, nlm_labels)\n",
    "# tuner.align_token_data(craft_spans, craft_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb6bd6d7-7660-41ef-a3f3-be0c47754421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_train_steps: 74340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at google/electra-base-discriminator were not used when initializing TFElectraForTokenClassification: ['discriminator_predictions']\n",
      "- This IS expected if you are initializing TFElectraForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFElectraForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFElectraForTokenClassification were not initialized from the model checkpoint at google/electra-base-discriminator and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "2230/2230 [==============================] - 275s 119ms/step - loss: 0.0870 - precision: 0.8792 - recall: 0.9009 - f1: 0.8899 - accuracy: 0.9880\n",
      "Epoch 2/30\n",
      "2230/2230 [==============================] - 266s 119ms/step - loss: 0.0315 - precision: 0.9079 - recall: 0.9202 - f1: 0.9140 - accuracy: 0.9901\n",
      "Epoch 3/30\n",
      "2230/2230 [==============================] - 265s 119ms/step - loss: 0.0204 - precision: 0.9120 - recall: 0.9449 - f1: 0.9282 - accuracy: 0.9908\n",
      "Epoch 4/30\n",
      "2230/2230 [==============================] - 266s 119ms/step - loss: 0.0151 - precision: 0.9210 - recall: 0.9428 - f1: 0.9318 - accuracy: 0.9921\n",
      "Epoch 5/30\n",
      "2230/2230 [==============================] - 265s 119ms/step - loss: 0.0100 - precision: 0.9291 - recall: 0.9447 - f1: 0.9368 - accuracy: 0.9924\n",
      "Epoch 6/30\n",
      "2230/2230 [==============================] - 266s 119ms/step - loss: 0.0078 - precision: 0.9324 - recall: 0.9442 - f1: 0.9383 - accuracy: 0.9924\n",
      "Epoch 7/30\n",
      "2230/2230 [==============================] - 265s 119ms/step - loss: 0.0067 - precision: 0.9061 - recall: 0.9559 - f1: 0.9303 - accuracy: 0.9896\n",
      "Epoch 8/30\n",
      "2230/2230 [==============================] - 265s 119ms/step - loss: 0.0047 - precision: 0.9296 - recall: 0.9554 - f1: 0.9423 - accuracy: 0.9927\n",
      "Epoch 9/30\n",
      "2230/2230 [==============================] - 265s 119ms/step - loss: 0.0041 - precision: 0.9357 - recall: 0.9523 - f1: 0.9439 - accuracy: 0.9925\n",
      "Epoch 10/30\n",
      "2230/2230 [==============================] - 266s 119ms/step - loss: 0.0029 - precision: 0.9341 - recall: 0.9480 - f1: 0.9410 - accuracy: 0.9926\n",
      "Epoch 11/30\n",
      "2230/2230 [==============================] - 266s 119ms/step - loss: 0.0029 - precision: 0.9350 - recall: 0.9522 - f1: 0.9435 - accuracy: 0.9930\n",
      "Epoch 12/30\n",
      "2230/2230 [==============================] - 266s 119ms/step - loss: 0.0021 - precision: 0.9324 - recall: 0.9625 - f1: 0.9472 - accuracy: 0.9930\n",
      "Epoch 13/30\n",
      "2230/2230 [==============================] - 267s 120ms/step - loss: 0.0018 - precision: 0.9305 - recall: 0.9586 - f1: 0.9443 - accuracy: 0.9928\n",
      "Epoch 14/30\n",
      "2230/2230 [==============================] - 267s 120ms/step - loss: 0.0019 - precision: 0.9312 - recall: 0.9524 - f1: 0.9417 - accuracy: 0.9926\n",
      "Epoch 15/30\n",
      "2230/2230 [==============================] - 266s 119ms/step - loss: 0.0013 - precision: 0.9491 - recall: 0.9472 - f1: 0.9481 - accuracy: 0.9931\n",
      "Epoch 16/30\n",
      "2230/2230 [==============================] - 266s 119ms/step - loss: 0.0015 - precision: 0.9382 - recall: 0.9555 - f1: 0.9468 - accuracy: 0.9928\n",
      "Epoch 17/30\n",
      "2230/2230 [==============================] - 266s 119ms/step - loss: 9.3434e-04 - precision: 0.9400 - recall: 0.9519 - f1: 0.9459 - accuracy: 0.9929\n",
      "Epoch 18/30\n",
      "2230/2230 [==============================] - 266s 119ms/step - loss: 9.2489e-04 - precision: 0.9413 - recall: 0.9551 - f1: 0.9481 - accuracy: 0.9934\n",
      "Epoch 19/30\n",
      "2230/2230 [==============================] - 266s 119ms/step - loss: 7.2718e-04 - precision: 0.9463 - recall: 0.9509 - f1: 0.9486 - accuracy: 0.9931\n",
      "Epoch 20/30\n",
      "2230/2230 [==============================] - 267s 120ms/step - loss: 8.4678e-04 - precision: 0.9403 - recall: 0.9562 - f1: 0.9482 - accuracy: 0.9935\n",
      "Epoch 21/30\n",
      "2230/2230 [==============================] - 266s 119ms/step - loss: 8.4878e-04 - precision: 0.9352 - recall: 0.9558 - f1: 0.9454 - accuracy: 0.9930\n",
      "Epoch 22/30\n",
      "2230/2230 [==============================] - 266s 119ms/step - loss: 4.8544e-04 - precision: 0.9406 - recall: 0.9556 - f1: 0.9481 - accuracy: 0.9931\n",
      "Epoch 23/30\n",
      "2230/2230 [==============================] - 266s 119ms/step - loss: 4.6357e-04 - precision: 0.9295 - recall: 0.9613 - f1: 0.9451 - accuracy: 0.9925\n",
      "Epoch 24/30\n",
      "2230/2230 [==============================] - 266s 119ms/step - loss: 5.3650e-04 - precision: 0.9396 - recall: 0.9539 - f1: 0.9467 - accuracy: 0.9931\n",
      "Epoch 25/30\n",
      "2230/2230 [==============================] - 266s 119ms/step - loss: 3.3826e-04 - precision: 0.9432 - recall: 0.9588 - f1: 0.9509 - accuracy: 0.9934\n",
      "Epoch 26/30\n",
      "2230/2230 [==============================] - 267s 120ms/step - loss: 3.2580e-04 - precision: 0.9415 - recall: 0.9588 - f1: 0.9501 - accuracy: 0.9934\n",
      "Epoch 27/30\n",
      "2230/2230 [==============================] - 268s 120ms/step - loss: 3.6513e-04 - precision: 0.9454 - recall: 0.9558 - f1: 0.9506 - accuracy: 0.9935\n",
      "Epoch 28/30\n",
      "2230/2230 [==============================] - 266s 119ms/step - loss: 1.8257e-04 - precision: 0.9480 - recall: 0.9520 - f1: 0.9500 - accuracy: 0.9935\n",
      "Epoch 29/30\n",
      "2230/2230 [==============================] - 266s 119ms/step - loss: 1.9413e-04 - precision: 0.9453 - recall: 0.9557 - f1: 0.9505 - accuracy: 0.9935\n",
      "Epoch 30/30\n",
      "2230/2230 [==============================] - 266s 119ms/step - loss: 1.6425e-04 - precision: 0.9442 - recall: 0.9582 - f1: 0.9511 - accuracy: 0.9936\n",
      "saving model to: ./model/chemical_extract_google-electra-base-discriminator\n",
      "Model: \"tf_electra_for_token_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " electra (TFElectraMainLaye  multiple                  108891648 \n",
      " r)                                                              \n",
      "                                                                 \n",
      " dropout_37 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  3845      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 108895493 (415.40 MB)\n",
      "Trainable params: 108895493 (415.40 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tuner.train(num_train_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c480ee8-bae3-48ba-9c06-0c7816368c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at ./model/chemical_extract_google-electra-base-discriminator were not used when initializing TFElectraForTokenClassification: ['dropout_37']\n",
      "- This IS expected if you are initializing TFElectraForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFElectraForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFElectraForTokenClassification were initialized from the model checkpoint at ./model/chemical_extract_google-electra-base-discriminator.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFElectraForTokenClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "To a suspension of <b style='color:red; font-size:1.2em;'><i>bis(2-chloroethyl)amine hydrochloride</i></b> (4.68 g, 26.2 mmol) in dry <b style='color:red; font-size:1.2em;'><i>THF</i></b> (50 mL) was added <b style='color:red; font-size:1.2em;'><i>Et3N</i></b> (18.3 mL, 131 mmol) at 0 °C and the mixture was stirred for 15 min. <b style='color:red; font-size:1.2em;'><i>p-Toluenesul-fonyl chloride</i></b> (5.00 g, 26.25 mmol) and <b style='color:red; font-size:1.2em;'><i>DMAP</i></b> (a spatula pinch) were added. The reqaction mixture was allowed to warm to room temperature and was stirred overnight. When TLC analysis showed complete conversion, the mixture was ﬁltered to remove the <b style='color:red; font-size:1.2em;'><i>Et3N4HCl</i></b> and extracted with <b style='color:red; font-size:1.2em;'><i>EtOAc</i></b>. The combined organic phases were washed with <b style='color:red; font-size:1.2em;'><i>brine</i></b>, dried (<b style='color:red; font-size:1.2em;'><i>Na2SO4</i></b>), ﬁltered, and concentrated. The residue was puriﬁed by ﬂash column chromatography to afford <b style='color:red; font-size:1.2em;'><i>N,N-bis(2-chlo-roethyl)-4-methylbenzenesulfonamide</i></b> (6.22 g,80% yield)."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "tuner.load_model(\"./model/chemical_extract_google-electra-base-discriminator\")\n",
    "\n",
    "html = tuner.infer_html(\"\"\"To a suspension of bis(2-chloroethyl)amine hydrochloride (4.68 g, 26.2 mmol) in dry THF (50 mL) was added Et3N (18.3 mL, 131 mmol) at 0 °C and the mixture was stirred for 15 min. p-Toluenesul-fonyl chloride (5.00 g, 26.25 mmol) and DMAP (a spatula pinch) were added. The reqaction mixture was allowed to warm to room temperature and was stirred overnight. When TLC analysis showed complete conversion, the mixture was ﬁltered to remove the Et3N4HCl and extracted with EtOAc. The combined organic phases were washed with brine, dried (Na2SO4), ﬁltered, and concentrated. The residue was puriﬁed by ﬂash column chromatography to afford N,N-bis(2-chlo-roethyl)-4-methylbenzenesulfonamide (6.22 g,80% yield).\"\"\")\n",
    "display(HTML(html))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c861166f-9d5a-46c2-ac82-dcc1b7e44d09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Deoxygenation of <b style='color:red; font-size:1.2em;'><i>nitrous oxide</i></b> (<b style='color:red; font-size:1.2em;'><i>N2O</i></b>) has significant environmental implications as it is not only a potent <b style='color:blue; font-size:1.5em;'><i>greenhouse gas</i></b> but is also the main substance responsible for the depletion of <b style='color:red; font-size:1.2em;'><i>ozone</i></b> in the stratosphere."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "html = tuner.infer_html(\"\"\"Deoxygenation of nitrous oxide (N2O) has significant environmental implications as it is not only a potent greenhouse gas but is also the main substance responsible for the depletion of ozone in the stratosphere.\"\"\")\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b22f450f-d0ad-4319-85b9-a018eda5bd94",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'craft' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[1;32m      2\u001b[0m nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men_core_web_lg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[43mcraft\u001b[49m\u001b[38;5;241m.\u001b[39mtext[\u001b[38;5;241m17069463\u001b[39m]\n\u001b[1;32m      5\u001b[0m doc \u001b[38;5;241m=\u001b[39m nlp(text)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m doc\u001b[38;5;241m.\u001b[39msents:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'craft' is not defined"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "text = craft.text[17069463]\n",
    "doc = nlp(text)\n",
    "for s in doc.sents:\n",
    "    html = tuner.infer_html(s.text)\n",
    "    display(HTML(html))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
