{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c4c563a-55e4-449d-bfaa-515528465933",
   "metadata": {},
   "source": [
    "## load the model which has been trained in the other notebook and use for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b2ee05c-1d59-48e2-a65d-f6cc288397ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"./loaders\")\n",
    "\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display, HTML\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "from ModelTuner import ModelTuner\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "model_name = \"google/electra-base-discriminator\"\n",
    "\n",
    "id2label = {\n",
    "    0: \"O\",\n",
    "    1: \"B-chemical\",\n",
    "    2: \"I-chemical\",\n",
    "    3: \"B-role\",\n",
    "    4: \"I-role\"\n",
    "}\n",
    "label2id = {\n",
    "    \"O\": 0,\n",
    "    \"B-chemical\": 1,\n",
    "    \"I-chemical\": 2,\n",
    "    \"B-role\": 3,\n",
    "    \"I-role\": 4,    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b117e19f-8000-4b17-a2ea-dc40c03c5f18",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFElectraForTokenClassification.\n",
      "\n",
      "All the layers of TFElectraForTokenClassification were initialized from the model checkpoint at /local/sps-local/cear-inferer/chemical_extract_google-electra-base-discriminator.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFElectraForTokenClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_electra_for_token_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " electra (TFElectraMainLaye  multiple                  108891648 \n",
      " r)                                                              \n",
      "                                                                 \n",
      " dropout_37 (Dropout)        multiple                  0 (unused)\n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  3845      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 108895493 (415.40 MB)\n",
      "Trainable params: 108895493 (415.40 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tuner = ModelTuner(model_name, list(label2id.keys()), id2label = id2label, label2id = label2id)\n",
    "tuner.load_model(f\"/local/sps-local/cear-inferer/chemical_extract_{model_name.replace('/', '-')}\")\n",
    "tuner.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "872ee67f-ca15-4d79-8421-0f6781699600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Biomolecules in microbes related to <b style='color:red; font-size:1.2em;'><i>CO2</i></b> -sensitive pathways or acting as a <b style='color:red; font-size:1.2em;'><i>CO2</i></b> trans-\n",
       "ducer have been proposed as appealing targets for <b style='color:blue; font-size:1.5em;'><i>medicines</i></b>, since they control cell devel-\n",
       "opment and the subsequent synthesis of chemicals, enhancing the pathogen persistence\n",
       "in the host [26,27]. In this context, a crucial role is played by a superfamily of molecules\n",
       "known as carbonic anhydrases (CAs, EC 4.2.1.1). CAs can be thought as molecules that,\n",
       "rather than instantly detecting a change in <b style='color:red; font-size:1.2em;'><i>CO2</i></b> , serve as <b style='color:red; font-size:1.2em;'><i>CO2</i></b> transducers, adjusting its\n",
       "levels [23,28]. With their activity, the CAs encoded by the bacterial genome of pathogenic\n",
       "and non-pathogenic bacteria provide the indispensable <b style='color:red; font-size:1.2em;'><i>CO2</i></b> and <b style='color:red; font-size:1.2em;'><i>HCO3 −</i></b> /protons to micro-\n",
       "bial biosynthetic pathways, catalyzing the reversible reaction of <b style='color:red; font-size:1.2em;'><i>CO2</i></b> hydration to <b style='color:red; font-size:1.2em;'><i>HCO3 −</i></b>\n",
       "and <b style='color:red; font-size:1.2em;'><i>H+</i></b>(<b style='color:red; font-size:1.2em;'><i>CO2+<b style='color:red; font-size:1.2em;'><i></i></b>H2OHCO3−</i></b>+<b style='color:red; font-size:1.2em;'><i>H+</i></b>)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"Biomolecules in microbes related to CO2 -sensitive pathways or acting as a CO2 trans-\n",
    "ducer have been proposed as appealing targets for medicines, since they control cell devel-\n",
    "opment and the subsequent synthesis of chemicals, enhancing the pathogen persistence\n",
    "in the host [26,27]. In this context, a crucial role is played by a superfamily of molecules\n",
    "known as carbonic anhydrases (CAs, EC 4.2.1.1). CAs can be thought as molecules that,\n",
    "rather than instantly detecting a change in CO2 , serve as CO2 transducers, adjusting its\n",
    "levels [23,28]. With their activity, the CAs encoded by the bacterial genome of pathogenic\n",
    "and non-pathogenic bacteria provide the indispensable CO2 and HCO3 − /protons to micro-\n",
    "bial biosynthetic pathways, catalyzing the reversible reaction of CO2 hydration to HCO3 −\n",
    "and H+(CO2+H2OHCO3−+H+)\"\"\"\n",
    "\n",
    "HTML(tuner.infer_html(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45c22459-0f9e-4892-9aa3-bf455caf3944",
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHED_ARTICLES_DIR = \"/local/sps-local/docs\"\n",
    "\n",
    "# read json document and return content as a json object\n",
    "def get_json_from_file(json_file):\n",
    "    with open(json_file, \"r\") as f:\n",
    "        return json.loads(f.read())\n",
    "    \n",
    "def recursively_collect_files():\n",
    "    filepaths = []\n",
    "    for root, dirs, files in os.walk(CACHED_ARTICLES_DIR):\n",
    "        for filename in files:            \n",
    "            if filename.endswith(\".json\") and not filename.endswith(\"-cear.json\"):                   \n",
    "                filepaths.append(os.path.join(root, filename))\n",
    "    return filepaths\n",
    "\n",
    "def collect_relevant_sentences(filepath):    \n",
    "    \"\"\"\n",
    "    collect sentences which have at least one chemical and one role\n",
    "    \"\"\"\n",
    "    sentences = []    \n",
    "    json_data = get_json_from_file(filepath)\n",
    "    filehash = json_data[\"fileHash\"]\n",
    "    contenthash = json_data[\"contentHash\"]\n",
    "    texthash = json_data[\"textHash\"]\n",
    "    origpath = json_data[\"filepath\"]\n",
    "\n",
    "    pages = [page for page in json_data[\"pages\"]]\n",
    "    \n",
    "    for page in pages:\n",
    "        doc = nlp(page[\"text\"])\n",
    "        for sentence in doc.sents:                        \n",
    "            page_number = int(page[\"pageNumber\"])\n",
    "            specials = tuner.infer(sentence.text)\n",
    "            contains_chem = False\n",
    "            contains_role = False\n",
    "            for s in specials:\n",
    "                if s[0] == 1:\n",
    "                    contains_chem = True\n",
    "                if s[0] % 2 == 1 and s[0] > 1:\n",
    "                    contains_role = True\n",
    "            if contains_chem and contains_role:                \n",
    "                sentences.append((filepath, page_number, doc[sentence.start].idx, specials, sentence.text))\n",
    "    return sentences\n",
    "     \n",
    "\n",
    "def load_relevant_sentences():\n",
    "    if os.path.isfile(\"/local/sps-local/ner-role-extraction/relevant_sentences.pkl\"):\n",
    "        with open(\"/local/sps-local/ner-role-extraction/relevant_sentences.pkl\", \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "def pickle_relevant_sentences():\n",
    "    with open(\"/local/sps-local/ner-role-extraction/relevant_sentences.pkl\", \"wb\") as f:\n",
    "        pickle.dump(sentences, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1a725e-82b7-42da-aead-5f363e1f3515",
   "metadata": {},
   "source": [
    "# offset and limit for training\n",
    "\n",
    "roughly 1000 files takes about 10 hours on a **NVIDIA RTX A5000** with **24 GB** of memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fba3c8f-2bde-47ef-849d-664bec487572",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                                                 | 0/693 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "filepaths = recursively_collect_files()\n",
    "\n",
    "offset = 0\n",
    "limit = 1000\n",
    "sentences = load_relevant_sentences()\n",
    "\n",
    "with open(\"/local/sps-local/ner-role-extraction/ner-role-inferer.log\", \"w\") as log:    \n",
    "    log.write(f\"{datetime.now()}: starting at offset {offset} and stopping at {offset+limit}\\n\")\n",
    "    log.flush()\n",
    "    for filepath in tqdm(filepaths[offset:offset+limit]):    \n",
    "        offset += 1\n",
    "        if offset%10 == 0:\n",
    "            log.write(f\"{datetime.now()}: attempting saving {offset}\\n\")\n",
    "            log.flush()\n",
    "            pickle_relevant_sentences()\n",
    "            log.write(f\"{datetime.now()}: done {offset}\\n\")\n",
    "            log.flush()\n",
    "        sentences.extend(collect_relevant_sentences(filepath))\n",
    "\n",
    "pickle_relevant_sentences()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
